{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 : Creating a function to remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>There are 3 ways we remove our stop words :</b>\n",
    "\n",
    "1. Considering all the words that are not in master dictionary as stop words\n",
    "2. Link to a website containing STOPWORDS files, can be used to a list of STOPWORDS\n",
    "   which we will find out that, it has 12K around STOPWORDS\n",
    "3. Using NLTK stopwords(it might not be a much of a preferred way because NLTK dosesn't have that many words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <b>USING MASTER DICTIONARY</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_keep = list(md['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dict_ini = pd.read_excel('LoughranMcDonald_MasterDictionary_2018.xlsx')\n",
    "\n",
    "md = master_dict_ini.copy()\n",
    "\n",
    "#md\n",
    "\n",
    "md['Word'] = md['Word'].apply(lambda x: str(x).lower())\n",
    "\n",
    "words_to_keep.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86486"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <b>OR: STOPWORDS FROM THE GIVEN WEBSITE</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "stop_word_file_path = '/home/pramila/Desktop/CLASSWORK/ML-ASGN-BLACKCOFFER/blackcoffer assignment/STOP-WORDS/'\n",
    "\n",
    "list_of_files = os.listdir(stop_word_file_path)\n",
    "\n",
    "#2\n",
    "for i in list_of_files:\n",
    "    \n",
    "    with open(os.path.join(stop_word_file_path, i), 'r') as words :\n",
    "        \n",
    "        content = words.read()\n",
    "        \n",
    "        with open('stop_words.txt', 'a+') as stop_words :\n",
    "            \n",
    "            stop_words.write(content + '\\n')\n",
    "            \n",
    "            \n",
    "#3\n",
    "with open('stop_words.txt', 'r') as stop_words :\n",
    "            \n",
    "            list_of_stop_words = stop_words.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_of_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some modification req in stop_words_list\n",
    "\n",
    "for i in range(len(list_of_stop_words)) :\n",
    "    \n",
    "    list_of_stop_words[i] = list_of_stop_words[i].replace('\\n', \"\").lower()\n",
    "\n",
    "\n",
    "list_of_stop_words = list(set(list_of_stop_words))\n",
    "\n",
    "list_of_stop_words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#list_of_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <b>OR: STOPWORDS FROM NLTK MODULE</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pramila/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <b>THE FUNCTION TO FILTER OUT STOPWORDS</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>MODE parameter</b> states which way you want to find stop words\n",
    "\n",
    "- <b>MODE = 0</b> : stopwords from website\n",
    "- <b>MODE = 1</b> : stopwords from nltk library\n",
    "- <b>MODE = 2</b> : using master dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(content, MODE=0):\n",
    "    \n",
    "    filtered_words = []\n",
    "    \n",
    "    if MODE == 0 :\n",
    "        \n",
    "        for i in content:\n",
    "\n",
    "            if i not in list_of_stop_words:\n",
    "\n",
    "                filtered_words.append(i)\n",
    "\n",
    "        return filtered_words\n",
    "    \n",
    "    if MODE == 1 :\n",
    "        \n",
    "        for i in content:\n",
    "\n",
    "            if i not in stop_words:\n",
    "\n",
    "                filtered_words.append(i)\n",
    "                \n",
    "        return filtered_words\n",
    "    \n",
    "    if MODE == 2 :\n",
    "        \n",
    "        for i in content:\n",
    "            \n",
    "            if i in words_to_keep:\n",
    "                \n",
    "                filtered_words.append(i)\n",
    "                \n",
    "        return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 : Extracting the (textual data + related variables) we need, from each financial report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2A - PREPARING DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c anaconda xlrd --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_report_ini_data = pd.read_excel('cik_list.xlsx') #original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_rep_data = financial_report_ini_data.copy() #copying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_rep_data['SECFNAME'] = 'https://www.sec.gov/Archives/' + fin_rep_data['SECFNAME'] #modifying the column F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#fin_rep_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2B - GETTING UNCERTAINITY AND CONSTRAINING WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting uncertain and constraining words \n",
    "\n",
    "uw_initial = pd.read_excel('uncertainty_dictionary.xlsx')\n",
    "cons_initial = pd.read_excel('constraining_dictionary.xlsx')\n",
    "\n",
    "uncertainity_words = uw_initial.copy()\n",
    "constraining_words = cons_initial.copy()\n",
    "\n",
    "#constraining_words\n",
    "\n",
    "uncertainity_words['Word'] = uncertainity_words['Word'].apply(lambda x: str(x).lower())\n",
    "constraining_words['Word'] = constraining_words['Word'].apply(lambda x: str(x).lower())\n",
    "\n",
    "uncertainity_words = list(uncertainity_words['Word'])\n",
    "constraining_words = list(constraining_words['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncertainity_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constraining_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - modifying FDATE column's datastructure, for better use of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas._libs.tslibs.timestamps.Timestamp"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fin_rep_data.loc[0, 'FDATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_rep_data['FDATE'] = fin_rep_data['FDATE'].apply(lambda x : datetime.date(x.year,x.month,x.day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fin_rep_data.loc[0, 'FDATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fin_rep_data.loc[0, 'FDATE'].year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D - EXTRACTING SECTION WISE INFORMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - <b>EXTRACTING CONTENT FROM URLS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = defaultdict(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in fin_rep_data.index:\n",
    "    \n",
    "    url[i] = fin_rep_data.loc[i, 'SECFNAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - <b>SECTIONS TO LOOK FOR</b> : \n",
    "Management's Discussion and Analysis, Quantitative and Qualitative Disclosures about Market Risk, Risk Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_patterns = [\n",
    "    r'''(ITEM\\s.\\.\\sMANAGEMENTS\\sDISCUSSION\\sAND\\sANALYSIS\\sOF\\sFINANCIAL\\sCONDITION\\sAND\\sRESULTS)(.*?)(ITEM\\s\\d\\.\\s)''', \n",
    "    r'''(ITEM\\s.\\.\\sQUANTITATIVE\\sAND\\sQUALITATIVE\\sDISCLOSURES\\sABOUT\\sMARKET\\sRISK)(.*?)(ITEM\\s.\\.\\s)''',\n",
    "    r'''(ITEM\\s.\\.\\sRISK\\sFACTORS\\s)(.*?)(ITEM\\s.\\.\\s)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <b>FUNCTIONS FOR MODIFICATIONS & GETTING VARIABLES' VALUES IN-HAND:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. <b>`modify_file_content`</b> \n",
    "is the function that remove unneccessary characters in whole file, and returns content in the form of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_file_content(l):\n",
    "    \n",
    "    #l will the list of lines in file and function returns a full content in form of single string\n",
    "    \n",
    "    for i in l:\n",
    "        i.replace('\\n', ' ')\n",
    "        i.replace('\\\\', '')\n",
    "        i.replace('\\t', ' ')\n",
    "        i.replace(\"\\'\", '')\n",
    "    \n",
    "    #new_str = ' '.join(l)\n",
    "    \n",
    "    #return new_str\n",
    "    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. <b>`clean`</b> \n",
    "is used on extracted out sections rather than whole file. It first removes more of unneccessary characters and the removes stopwords. It returns list of remaining words that are useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(actual_content):\n",
    "    \n",
    "    # 1. UNNECCESSARY CHARACTERS REMOVAL\n",
    "    \n",
    "    actual_content = re.sub(r\"(<.*>)\",r\" \",actual_content).lower()\n",
    "    \n",
    "    actual_content = re.sub(\"[^a-zA-Z]\",\" \",actual_content).lower()\n",
    "\n",
    "    actual_content = re.sub(r'\\s+', r' ', actual_content).lower() \n",
    "    \n",
    "    # 2. REMOVING STOP WORDS\n",
    "    \n",
    "    actual_content = list(set(actual_content.split()))\n",
    "        \n",
    "    actual_content = remove_stop_words(actual_content, MODE=0)\n",
    "    \n",
    "    return actual_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. <b>`section_content_info`</b> \n",
    "is the function that extracts out particular sections from whole file, based on regex pattern provided and applies cleaning with `clean` function. It return number of sentences and useful words from the excerpt. It returns None if that particular section is not found in particular file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def section_content_info(content_string, pattern):\n",
    "    \n",
    "    temp_list = re.findall(pattern, content_string, re.M)\n",
    "    \n",
    "    if len(temp_list) != 0 :\n",
    "\n",
    "        actual_content = temp_list[0][1]\n",
    "\n",
    "        number_of_sentences = len(sent_tokenize(actual_content))\n",
    "        \n",
    "        actual_content = clean(actual_content)\n",
    "        \n",
    "        # NLTK TOKENIZER\n",
    "\n",
    "        #    IT CAN BE NOTED THAT OUR TEXT STRING IS TOKENIZED UPTILL NOW\n",
    "        #    WE DON'T NEED NLTK TOKENIZER, BUT STILL, JUST IN CASE WE\n",
    "        #    HAVE MISSED SOMETHING, NLTK TOKENIZER WILL HANDLE THAT !\n",
    "\n",
    "        actual_content = ' '.join(actual_content)\n",
    "\n",
    "        final_words = word_tokenize(actual_content)\n",
    "\n",
    "        return [[final_words], number_of_sentences]\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return [None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. <b>`analysis_whole_report`</b> \n",
    "is the function that finds the number of constraining words for a given file, which needs to be added as last variable in our final output data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_whole_report(content):\n",
    "    \n",
    "    new_content = clean(content)\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for i in new_content :\n",
    "        \n",
    "        if i in constraining_words :\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>THE MAIN LOOP</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0 \n",
    "\n",
    "mda = defaultdict(list)\n",
    "qqdmr = defaultdict(list)\n",
    "#rf = default_dict()\n",
    "\n",
    "constraining_words_whole_report = defaultdict()\n",
    "\n",
    "for i in url :\n",
    "    \n",
    "    # 1. getting the content of the url\n",
    "    \n",
    "    response = urllib.request.urlopen(url[i])\n",
    "    content = response.read().decode('utf8')\n",
    "    \n",
    "    # 2. writing the url content into a file\n",
    "    \n",
    "#     with open('content.txt', 'w+') as f:\n",
    "#         f.write(raw)\n",
    "\n",
    "    # 3. list of lines\n",
    "    \n",
    "#     with open(\"content.txt\",\"r\") as f:\n",
    "#         content = f.readlines()\n",
    "        \n",
    "    # 4. making the content into better form\n",
    "    \n",
    "    content = modify_file_content(content)\n",
    "    \n",
    "    # 5. Extract text for each section\n",
    "    \n",
    "    mda[c] = section_content_info(content, sections_patterns[0])\n",
    "    \n",
    "    qqdmr[c] = section_content_info(content, sections_patterns[1])\n",
    "    \n",
    "    #rq[c] = section_content_info(content, sections_patterns[2])\n",
    "    \n",
    "    constraining_words_whole_report[c] = analysis_whole_report(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.sec.gov/Archives/edgar/data/12239/0001104659-07-062470.txt'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw = modify_file_content(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <b>OTHER SCORES</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying the index of master dictionary\n",
    "\n",
    "md = md.set_index(['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence Number</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word Proportion</th>\n",
       "      <th>Average Proportion</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Doc Count</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>Constraining</th>\n",
       "      <th>Superfluous</th>\n",
       "      <th>Interesting</th>\n",
       "      <th>Modal</th>\n",
       "      <th>Irr_Verb</th>\n",
       "      <th>Harvard_IV</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aardvark</th>\n",
       "      <td>1</td>\n",
       "      <td>277</td>\n",
       "      <td>1.480368e-08</td>\n",
       "      <td>1.239377e-08</td>\n",
       "      <td>3.564730e-06</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aardvarks</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.603287e-10</td>\n",
       "      <td>9.725110e-12</td>\n",
       "      <td>9.863549e-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abaci</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4.275431e-10</td>\n",
       "      <td>1.386497e-10</td>\n",
       "      <td>6.225591e-08</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aback</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>6.413147e-10</td>\n",
       "      <td>3.159061e-10</td>\n",
       "      <td>9.383557e-08</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abacus</th>\n",
       "      <td>5</td>\n",
       "      <td>7250</td>\n",
       "      <td>3.874610e-07</td>\n",
       "      <td>3.681624e-07</td>\n",
       "      <td>3.366553e-05</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Sequence Number  Word Count  Word Proportion  Average Proportion  \\\n",
       "Word                                                                          \n",
       "aardvark                 1         277     1.480368e-08        1.239377e-08   \n",
       "aardvarks                2           3     1.603287e-10        9.725110e-12   \n",
       "abaci                    3           8     4.275431e-10        1.386497e-10   \n",
       "aback                    4          12     6.413147e-10        3.159061e-10   \n",
       "abacus                   5        7250     3.874610e-07        3.681624e-07   \n",
       "\n",
       "                Std Dev  Doc Count  Negative  Positive  Uncertainty  \\\n",
       "Word                                                                  \n",
       "aardvark   3.564730e-06         84         0         0            0   \n",
       "aardvarks  9.863549e-09          1         0         0            0   \n",
       "abaci      6.225591e-08          7         0         0            0   \n",
       "aback      9.383557e-08         12         0         0            0   \n",
       "abacus     3.366553e-05        914         0         0            0   \n",
       "\n",
       "           Litigious  Constraining  Superfluous  Interesting  Modal  Irr_Verb  \\\n",
       "Word                                                                            \n",
       "aardvark           0             0            0            0      0         0   \n",
       "aardvarks          0             0            0            0      0         0   \n",
       "abaci              0             0            0            0      0         0   \n",
       "aback              0             0            0            0      0         0   \n",
       "abacus             0             0            0            0      0         0   \n",
       "\n",
       "           Harvard_IV  Syllables     Source  \n",
       "Word                                         \n",
       "aardvark            0          2  12of12inf  \n",
       "aardvarks           0          2  12of12inf  \n",
       "abaci               0          3  12of12inf  \n",
       "aback               0          2  12of12inf  \n",
       "abacus              0          3  12of12inf  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sequence Number', 'Word Count', 'Word Proportion',\n",
       "       'Average Proportion', 'Std Dev', 'Doc Count', 'Negative', 'Positive',\n",
       "       'Uncertainty', 'Litigious', 'Constraining', 'Superfluous',\n",
       "       'Interesting', 'Modal', 'Irr_Verb', 'Harvard_IV', 'Syllables',\n",
       "       'Source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#md.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE FUNCTION CALCULATING SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(words, number_of_sentences):\n",
    "    \n",
    "    negative_score = 0\n",
    "    \n",
    "    positive_score = 0\n",
    "    \n",
    "    complex_word_count = 0\n",
    "    \n",
    "    word_count = len(words)\n",
    "    \n",
    "    word_length = 0\n",
    "    \n",
    "    uncertainty_score = 0\n",
    "    \n",
    "    constraining_score = 0\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        word_length += len(word)\n",
    "        \n",
    "        if md.loc[word, 'Syllables'] >2 :\n",
    "            \n",
    "            complex_word_count +=1\n",
    "        \n",
    "        if md.loc[word, 'Positive'] != 0:\n",
    "            \n",
    "            positive_score += 1\n",
    "        \n",
    "        if md.loc[word, 'Negative'] != 0:\n",
    "            \n",
    "            negative_score += 1\n",
    "    \n",
    "        if word in uncertainity_words:\n",
    "            \n",
    "            uncertainty_score += 1\n",
    "            \n",
    "        if word in constraining_words:\n",
    "            \n",
    "            constraining_score += 1\n",
    "            \n",
    "    polarity_score = (positive_score - negative_score)/((positive_score + negative_score) + 0.000001)\n",
    "    \n",
    "    subjectivity_score = (positive_score + negative_score)/(len(words) + 0.000001)\n",
    "\n",
    "    average_sentence_length = len(words)/number_of_sentences\n",
    "    \n",
    "    percentage_of_complex_words = complex_word_count / len(words)\n",
    "    \n",
    "    fog_index = (0.4)*(average_sentence_length + percentage_of_complex_words)\n",
    "    \n",
    "    average_word_length = word_length / len(words)\n",
    "    \n",
    "    positive_word_proportion = positive_score / word_count\n",
    "    \n",
    "    negative_word_proportion = negative_score / word_count\n",
    "    \n",
    "    uncertainty_word_proportion = uncertainty_score / word_count\n",
    "    \n",
    "    constraining_word_proportion = constraining_score / words_count\n",
    "    \n",
    "    return (positive_score, \n",
    "            negative_score, \n",
    "            average_sentence_length, \n",
    "            percentage_of_complex_words, \n",
    "            fog_index, \n",
    "            complex_word_count,\n",
    "            word_count,\n",
    "            uncertainty_score,\n",
    "            constraining_score,\n",
    "            positive_word_proportion,\n",
    "            negative_word_proportion,\n",
    "            uncertainty_word_proportion,\n",
    "            constraining_word_proportion\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_variables():\n",
    "    \n",
    "    for i in mda.keys():\n",
    "        (mda_positive_score[i],\n",
    "        mda_negative_score[i],\n",
    "        mda_polarity_score[i],\n",
    "        mda_average_sentence_length[i],\n",
    "        mda_percentage_of_complex_words[i],\n",
    "        mda_fog_index[i],\n",
    "        mda_complex_word_count[i],\n",
    "        mda_word_count[i],\n",
    "        mda_uncertainty_score[i],\n",
    "        mda_constraining_score[i],\n",
    "        mda_positive_word_proportion[i],\n",
    "        mda_negative_word_proportion[i],\n",
    "        mda_uncertainty_word_proportion[i],\n",
    "        mda_constraining_word_proportion[i]) = scores(mda[i])\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = urllib.request.urlopen(url)\n",
    "\n",
    "# raw = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_filename, headers = urllib.request.urlretrieve('https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-000413.txt')\n",
    "# html = open(local_filename).decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sections = [\"Management's Discussion and Analysis\", \"Quantitative and Qualitative Disclosures about Market Risk\", \"Risk Factors\"]\n",
    "\n",
    "# z = html.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f= open(\"temp.txt\",\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.write(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('temp.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# print(f[:-1])\n",
    "        \n",
    "# f = open('temp_file.txt', 'w')\n",
    "\n",
    "# f.write(z)\n",
    "\n",
    "# k = open('temp_file.txt', 'r')\n",
    "\n",
    "# k.readlines(1)\n",
    "\n",
    "# k.close()\n",
    "\n",
    "# f.close()\n",
    "    \n",
    "#     print(type(html))\n",
    "            \n",
    "#     with html as fileinput:\n",
    "        \n",
    "#         print(fileinput)\n",
    "        \n",
    "#         exit\n",
    "        \n",
    "#         for line in fileinput:\n",
    "            \n",
    "#             line = line.lower()\n",
    "        \n",
    "#         if '\\n' in  fileinput:\n",
    "            \n",
    "#             print('yes')\n",
    "\n",
    "# f.close()\n",
    "\n",
    "# f = open('temp_file.txt' 'r')\n",
    "\n",
    "# f.read()\n",
    "\n",
    "# f.close()\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-000413.txt\"\n",
    "# response = urllib.request.urlopen(url)\n",
    "# raw = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex = r\"(?<=ITEM\\s\\d\\.\\sMANAGEMENT\\'S\\sDISCUSSION\\sAND\\sANALYSIS\\sOF\\sFINANCIAL\\sCONDITION\\sAND\\sRESULTS\\sOF\\sOPERATION)(.*?)(?=ITEM\\s\\d\\.\\s)\"\n",
    "\n",
    "#test_str = (\"ITEM 7. MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATION \\nkdfoks[dlsaf;\\nsd\\nlgsld\\ns['a\\n'[df\\na'fd\\nsaD\\FSAD\\nfF\\n ITEM 8. dhfkjskaskdkskd\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re.findall(regex, test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for i in range(len(l)):\n",
    "#     l[i] = l[i].replace('\\n', ' ')\n",
    "#     l[i] = l[i].replace('\\\\', '')\n",
    "#     l[i] = l[i].replace('\\t', ' ')\n",
    "#     l[i] = l[i].replace(\"\\'\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_str = ' '.join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern = r'''(?<=ITEM\\s\\d\\.\\sMANAGEMENTS\\sDISCUSSION\\sAND\\sANALYSIS\\sOF\\sFINANCIAL\\sCONDITION\\sAND\\sRESULTS\\sOF\\sOPERATION)(.*?)(?=ITEM\\s\\d\\.\\s)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = r'''(ITEM\\s\\d\\.\\sMANAGEMENTS\\sDISCUSSION\\sAND\\sANALYSIS\\sOF\\sFINANCIAL\\sCONDITION\\sAND\\sRESULTS)(.*?)(ITEM\\s\\d\\.\\s)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# new_list = re.findall(pattern, new_str, re.M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# len(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_text = new_list[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# final_text = re.sub(r'\\s+', r' ', final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_text = re.sub(r\"(<.*>)\",r\" \",final_text).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_text = re.sub(\"[^a-zA-Z]\",\" \",final_text).lower()\n",
    "\n",
    "# final_text\n",
    "\n",
    "# final_text = re.sub(r'\\s+', r' ', final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_stop_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #1\n",
    "# stop_word_file_path = '/home/pramila/Desktop/CLASSWORK/ML-ASGN-BLACKCOFFER/blackcoffer assignment/STOP-WORDS/'\n",
    "\n",
    "# list_of_files = os.listdir(stop_word_file_path)\n",
    "\n",
    "# #2\n",
    "# for i in list_of_files:\n",
    "    \n",
    "#     with open(os.path.join(stop_word_file_path, i), 'r') as words :\n",
    "        \n",
    "#         content = words.read()\n",
    "        \n",
    "#         with open('stop_words.txt', 'a+') as stop_words :\n",
    "            \n",
    "#             stop_words.write(content + '\\n')\n",
    "            \n",
    "            \n",
    "# #3\n",
    "# with open('stop_words.txt', 'r') as stop_words :\n",
    "            \n",
    "#             list_of_stop_words = stop_words.readlines()\n",
    "\n",
    "# #list_of_stop_words\n",
    "\n",
    "# #some modification req in stop_words_list\n",
    "\n",
    "# for i in range(len(list_of_stop_words)) :\n",
    "    \n",
    "#     list_of_stop_words[i] = list_of_stop_words[i].replace('\\n', \"\").lower()\n",
    "\n",
    "\n",
    "# list_of_stop_words = list(set(list_of_stop_words))\n",
    "\n",
    "# list_of_stop_words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if polarity_score < -0.5 :\n",
    "        \n",
    "#         sentiment_score_categorization = 'Most Negative'\n",
    "        \n",
    "#     elif (polarity_score < 0) and (polarity_score > -0.5):\n",
    "        \n",
    "#         sentiment_score_categorization = 'Negative'\n",
    "        \n",
    "#     elif polarity_score == 0:\n",
    "        \n",
    "#         sentiment_score_categorization = 'Neutral'\n",
    "        \n",
    "#     elif (polarity_score > 0) and (polarity_score < 0.5):\n",
    "        \n",
    "#         sentiment_score_categorization = 'Positive'\n",
    "        \n",
    "#     elif polarity_score > 0.5:\n",
    "        \n",
    "#         sentiment_score_categorization = 'Very Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_stop_words(content):\n",
    "#     new_content = content\n",
    "#     print(len(content))\n",
    "    \n",
    "#     print(\"\\n\")\n",
    "#     for i in range(len(content)):\n",
    "#         print(i)\n",
    "        \n",
    "#         if content[i] in li1:\n",
    "        \n",
    "#             del new_content[i]\n",
    "            \n",
    "#         elif content[i] in li2:\n",
    "            \n",
    "#             del new_content[i]\n",
    "            \n",
    "#         elif content[i] in li3:\n",
    "            \n",
    "#             del new_content[i]\n",
    "            \n",
    "#         elif content[i] in li4:\n",
    "            \n",
    "#             del new_content[i]\n",
    "            \n",
    "#     return new_content\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words_string = \" \".join(list_of_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_text = re.split(r' ', final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_text = set(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import master dictionary\n",
    "\n",
    "# master_dict_ini = pd.read_excel('LoughranMcDonald_MasterDictionary_2018.xlsx')\n",
    "\n",
    "# md = master_dict_ini.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# md['Positive'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# md['Negative'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# md['Source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# sentences = 'A Turning machine is a device that manipulates symbols on a strip of tape according to a table of rules. Despite its simplicity, a Turing machine can be adapted to simulate the logic of any computer algorithm, and is particularly useful in explaining the functions of a CPU.... inside a computer. The \"Turing\" machine was described by Alan Turing in 1936, who called it an \"a(utomatic)-machine\". The Turing machine is not intended as a practical computing technology, but rather as a hypothetical device representing a computing machine. Turing machines help computer scientists understand the limits of mechaniacl computation.'\n",
    "\n",
    "# number_of_sentences = sent_tokenize(sentences)\n",
    "\n",
    "# print(len(number_of_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# md['Word'] = md['Word'].apply(lambda x: str(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_word_length = 0\n",
    "# total_words = md['Word'].shape[0]\n",
    "# all_lengths = []\n",
    "# for i in md['Word']:\n",
    "#     total_word_length += len(i)\n",
    "#     all_lengths.append(len(i))\n",
    "#     if (len(i) == 3 or len(i) == 24) :\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min(all_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_word_length/total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
