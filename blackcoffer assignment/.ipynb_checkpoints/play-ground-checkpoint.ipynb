{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 : Creating a function to remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>There are 3 ways we remove our stop words :</b>\n",
    "\n",
    "1. Considering all the words that are not in master dictionary as stop words\n",
    "2. Link to a website containing STOPWORDS files, can be used to a list of STOPWORDS\n",
    "   which we will find out that, it has 12K around STOPWORDS\n",
    "3. Using NLTK stopwords(it might not be a much of a preferred way because NLTK dosesn't have that many words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <b>USING MASTER DICTIONARY</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_keep = list(md['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dict_ini = pd.read_excel('LoughranMcDonald_MasterDictionary_2018.xlsx')\n",
    "\n",
    "md = master_dict_ini.copy()\n",
    "\n",
    "#md\n",
    "\n",
    "md['Word'] = md['Word'].apply(lambda x: str(x).lower())\n",
    "\n",
    "words_to_keep.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86486"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <b>OR: STOPWORDS FROM THE GIVEN WEBSITE</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "stop_word_file_path = '/home/pramila/Desktop/CLASSWORK/ML-ASGN-BLACKCOFFER/blackcoffer assignment/STOP-WORDS/'\n",
    "\n",
    "list_of_files = os.listdir(stop_word_file_path)\n",
    "\n",
    "#2\n",
    "for i in list_of_files:\n",
    "    \n",
    "    with open(os.path.join(stop_word_file_path, i), 'r') as words :\n",
    "        \n",
    "        content = words.read()\n",
    "        \n",
    "        with open('stop_words.txt', 'a+') as stop_words :\n",
    "            \n",
    "            stop_words.write(content + '\\n')\n",
    "            \n",
    "            \n",
    "#3\n",
    "with open('stop_words.txt', 'r') as stop_words :\n",
    "            \n",
    "            list_of_stop_words = stop_words.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_of_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some modification req in stop_words_list\n",
    "\n",
    "for i in range(len(list_of_stop_words)) :\n",
    "    \n",
    "    list_of_stop_words[i] = list_of_stop_words[i].replace('\\n', \"\").lower()\n",
    "\n",
    "\n",
    "list_of_stop_words = list(set(list_of_stop_words))\n",
    "\n",
    "list_of_stop_words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#list_of_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <b>OR: STOPWORDS FROM NLTK MODULE</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pramila/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <b>THE FUNCTION TO FILTER OUT STOPWORDS</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>MODE parameter</b> states which way you want to find stop words\n",
    "\n",
    "- <b>MODE = 0</b> : stopwords from website\n",
    "- <b>MODE = 1</b> : stopwords from nltk library\n",
    "- <b>MODE = 2</b> : using master dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(content, MODE=0):\n",
    "    \n",
    "    filtered_words = []\n",
    "    \n",
    "    if MODE == 0 :\n",
    "        \n",
    "        for i in content:\n",
    "\n",
    "            if i not in list_of_stop_words:\n",
    "\n",
    "                filtered_words.append(i)\n",
    "\n",
    "        return filtered_words\n",
    "    \n",
    "    if MODE == 1 :\n",
    "        \n",
    "        for i in content:\n",
    "\n",
    "            if i not in stop_words:\n",
    "\n",
    "                filtered_words.append(i)\n",
    "                \n",
    "        return filtered_words\n",
    "    \n",
    "    if MODE == 2 :\n",
    "        \n",
    "        for i in content:\n",
    "            \n",
    "            if i in words_to_keep:\n",
    "                \n",
    "                filtered_words.append(i)\n",
    "                \n",
    "        return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 : Extracting the (textual data + related variables) we need, from each financial report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2A - PREPARING DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c anaconda xlrd --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_report_ini_data = pd.read_excel('cik_list.xlsx') #original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_rep_data = financial_report_ini_data.copy() #copying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_rep_data['SECFNAME'] = 'https://www.sec.gov/Archives/' + fin_rep_data['SECFNAME'] #modifying the column F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#fin_rep_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2B - GETTING UNCERTAINITY AND CONSTRAINING WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting uncertain and constraining words \n",
    "\n",
    "uw_initial = pd.read_excel('uncertainty_dictionary.xlsx')\n",
    "cons_initial = pd.read_excel('constraining_dictionary.xlsx')\n",
    "\n",
    "uncertainity_words = uw_initial.copy()\n",
    "constraining_words = cons_initial.copy()\n",
    "\n",
    "#constraining_words\n",
    "\n",
    "uncertainity_words['Word'] = uncertainity_words['Word'].apply(lambda x: str(x).lower())\n",
    "constraining_words['Word'] = constraining_words['Word'].apply(lambda x: str(x).lower())\n",
    "\n",
    "uncertainity_words = list(uncertainity_words['Word'])\n",
    "constraining_words = list(constraining_words['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncertainity_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constraining_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - modifying FDATE column's datastructure, for better use of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas._libs.tslibs.timestamps.Timestamp"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fin_rep_data.loc[0, 'FDATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_rep_data['FDATE'] = fin_rep_data['FDATE'].apply(lambda x : datetime.date(x.year,x.month,x.day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fin_rep_data.loc[0, 'FDATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fin_rep_data.loc[0, 'FDATE'].year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D - EXTRACTING SECTION WISE INFORMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - <b>EXTRACTING CONTENT FROM URLS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = defaultdict(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in fin_rep_data.index:\n",
    "    \n",
    "    url[i] = fin_rep_data.loc[i, 'SECFNAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - <b>SECTIONS TO LOOK FOR</b> : \n",
    "Management's Discussion and Analysis, Quantitative and Qualitative Disclosures about Market Risk, Risk Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_patterns = [\n",
    "    r'''(ITEM\\s.\\.\\sMANAGEMENTS\\sDISCUSSION\\sAND\\sANALYSIS\\sOF\\sFINANCIAL\\sCONDITION\\sAND\\sRESULTS)(.*?)(ITEM\\s\\d\\.\\s)''', \n",
    "    r'''(ITEM\\s.\\.\\sQUANTITATIVE\\sAND\\sQUALITATIVE\\sDISCLOSURES\\sABOUT\\sMARKET\\sRISK)(.*?)(ITEM\\s.\\.\\s)''',\n",
    "    r'''(ITEM\\s.\\.\\sRISK\\sFACTORS\\s)(.*?)(ITEM\\s.\\.\\s)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <b>FUNCTIONS FOR MODIFICATIONS & GETTING VARIABLES' VALUES IN-HAND:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. <b>`modify_file_content`</b> \n",
    "is the function that remove unneccessary characters in whole file, and returns content in the form of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_file_content(l):\n",
    "    \n",
    "    #l will the list of lines in file and function returns a full content in form of single string\n",
    "    \n",
    "    for i in l:\n",
    "        i.replace('\\n', ' ')\n",
    "        i.replace('\\\\', '')\n",
    "        i.replace('\\t', ' ')\n",
    "        i.replace(\"\\'\", '')\n",
    "    \n",
    "    #new_str = ' '.join(l)\n",
    "    \n",
    "    #return new_str\n",
    "    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. <b>`clean`</b> \n",
    "is used on extracted out sections rather than whole file. It first removes more of unneccessary characters and the removes stopwords. It returns list of remaining words that are useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(actual_content):\n",
    "    \n",
    "    # 1. UNNECCESSARY CHARACTERS REMOVAL\n",
    "    \n",
    "    actual_content = re.sub(r\"(<.*>)\",r\" \",actual_content).lower()\n",
    "    \n",
    "    actual_content = re.sub(\"[^a-zA-Z]\",\" \",actual_content).lower()\n",
    "\n",
    "    actual_content = re.sub(r'\\s+', r' ', actual_content).lower() \n",
    "    \n",
    "    # 2. REMOVING STOP WORDS\n",
    "    \n",
    "    actual_content = list(set(actual_content.split()))\n",
    "        \n",
    "    actual_content = remove_stop_words(actual_content, MODE=0)\n",
    "    \n",
    "    return actual_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. <b>`section_content_info`</b> \n",
    "is the function that extracts out particular sections from whole file, based on regex pattern provided and applies cleaning with `clean` function. It return number of sentences and useful words from the excerpt. It returns None if that particular section is not found in particular file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def section_content_info(content_string, pattern):\n",
    "    \n",
    "    temp_list = re.findall(pattern, content_string, re.M)\n",
    "    \n",
    "    if len(temp_list) != 0 :\n",
    "\n",
    "        actual_content = temp_list[0][1]\n",
    "\n",
    "        number_of_sentences = len(sent_tokenize(actual_content))\n",
    "        \n",
    "        actual_content = clean(actual_content)\n",
    "        \n",
    "        # NLTK TOKENIZER\n",
    "\n",
    "        #    IT CAN BE NOTED THAT OUR TEXT STRING IS TOKENIZED UPTILL NOW\n",
    "        #    WE DON'T NEED NLTK TOKENIZER, BUT STILL, JUST IN CASE WE\n",
    "        #    HAVE MISSED SOMETHING, NLTK TOKENIZER WILL HANDLE THAT !\n",
    "\n",
    "        actual_content = ' '.join(actual_content)\n",
    "\n",
    "        final_words = word_tokenize(actual_content)\n",
    "\n",
    "        return [[final_words], number_of_sentences]\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return [None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. <b>`analysis_whole_report`</b> \n",
    "is the function that finds the number of constraining words for a given file, which needs to be added as last variable in our final output data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_whole_report(content):\n",
    "    \n",
    "    new_content = clean(content)\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for i in new_content :\n",
    "        \n",
    "        if i in constraining_words :\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>THE MAIN LOOP</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0 \n",
    "\n",
    "mda = defaultdict(list)\n",
    "qqdmr = defaultdict(list)\n",
    "#rf = default_dict()\n",
    "\n",
    "constraining_words_whole_report = defaultdict()\n",
    "\n",
    "for i in url :\n",
    "    \n",
    "    # 1. getting the content of the url\n",
    "    \n",
    "    response = urllib.request.urlopen(url[i])\n",
    "    content = response.read().decode('utf8')\n",
    "    \n",
    "    # 2. writing the url content into a file\n",
    "    \n",
    "#     with open('content.txt', 'w+') as f:\n",
    "#         f.write(raw)\n",
    "\n",
    "    # 3. list of lines\n",
    "    \n",
    "#     with open(\"content.txt\",\"r\") as f:\n",
    "#         content = f.readlines()\n",
    "        \n",
    "    # 4. making the content into better form\n",
    "    \n",
    "    content = modify_file_content(content)\n",
    "    \n",
    "    # 5. Extract text for each section\n",
    "    \n",
    "    mda[c] = section_content_info(content, sections_patterns[0])\n",
    "    \n",
    "    qqdmr[c] = section_content_info(content, sections_patterns[1])\n",
    "    \n",
    "    #rq[c] = section_content_info(content, sections_patterns[2])\n",
    "    \n",
    "    constraining_words_whole_report[c] = analysis_whole_report(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.sec.gov/Archives/edgar/data/12239/0001104659-07-062470.txt'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw = modify_file_content(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <b>OTHER SCORES</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying the index of master dictionary\n",
    "\n",
    "md = md.set_index(['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence Number</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word Proportion</th>\n",
       "      <th>Average Proportion</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Doc Count</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>Constraining</th>\n",
       "      <th>Superfluous</th>\n",
       "      <th>Interesting</th>\n",
       "      <th>Modal</th>\n",
       "      <th>Irr_Verb</th>\n",
       "      <th>Harvard_IV</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aardvark</th>\n",
       "      <td>1</td>\n",
       "      <td>277</td>\n",
       "      <td>1.480368e-08</td>\n",
       "      <td>1.239377e-08</td>\n",
       "      <td>3.564730e-06</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aardvarks</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.603287e-10</td>\n",
       "      <td>9.725110e-12</td>\n",
       "      <td>9.863549e-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abaci</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4.275431e-10</td>\n",
       "      <td>1.386497e-10</td>\n",
       "      <td>6.225591e-08</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aback</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>6.413147e-10</td>\n",
       "      <td>3.159061e-10</td>\n",
       "      <td>9.383557e-08</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abacus</th>\n",
       "      <td>5</td>\n",
       "      <td>7250</td>\n",
       "      <td>3.874610e-07</td>\n",
       "      <td>3.681624e-07</td>\n",
       "      <td>3.366553e-05</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Sequence Number  Word Count  Word Proportion  Average Proportion  \\\n",
       "Word                                                                          \n",
       "aardvark                 1         277     1.480368e-08        1.239377e-08   \n",
       "aardvarks                2           3     1.603287e-10        9.725110e-12   \n",
       "abaci                    3           8     4.275431e-10        1.386497e-10   \n",
       "aback                    4          12     6.413147e-10        3.159061e-10   \n",
       "abacus                   5        7250     3.874610e-07        3.681624e-07   \n",
       "\n",
       "                Std Dev  Doc Count  Negative  Positive  Uncertainty  \\\n",
       "Word                                                                  \n",
       "aardvark   3.564730e-06         84         0         0            0   \n",
       "aardvarks  9.863549e-09          1         0         0            0   \n",
       "abaci      6.225591e-08          7         0         0            0   \n",
       "aback      9.383557e-08         12         0         0            0   \n",
       "abacus     3.366553e-05        914         0         0            0   \n",
       "\n",
       "           Litigious  Constraining  Superfluous  Interesting  Modal  Irr_Verb  \\\n",
       "Word                                                                            \n",
       "aardvark           0             0            0            0      0         0   \n",
       "aardvarks          0             0            0            0      0         0   \n",
       "abaci              0             0            0            0      0         0   \n",
       "aback              0             0            0            0      0         0   \n",
       "abacus             0             0            0            0      0         0   \n",
       "\n",
       "           Harvard_IV  Syllables     Source  \n",
       "Word                                         \n",
       "aardvark            0          2  12of12inf  \n",
       "aardvarks           0          2  12of12inf  \n",
       "abaci               0          3  12of12inf  \n",
       "aback               0          2  12of12inf  \n",
       "abacus              0          3  12of12inf  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sequence Number', 'Word Count', 'Word Proportion',\n",
       "       'Average Proportion', 'Std Dev', 'Doc Count', 'Negative', 'Positive',\n",
       "       'Uncertainty', 'Litigious', 'Constraining', 'Superfluous',\n",
       "       'Interesting', 'Modal', 'Irr_Verb', 'Harvard_IV', 'Syllables',\n",
       "       'Source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#md.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE FUNCTION CALCULATING SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(words, number_of_sentences):\n",
    "    \n",
    "    negative_score = 0\n",
    "    \n",
    "    positive_score = 0\n",
    "    \n",
    "    complex_word_count = 0\n",
    "    \n",
    "    word_count = len(words)\n",
    "    \n",
    "    word_length = 0\n",
    "    \n",
    "    uncertainty_score = 0\n",
    "    \n",
    "    constraining_score = 0\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        word_length += len(word)\n",
    "        \n",
    "        if md.loc[word, 'Syllables'] >2 :\n",
    "            \n",
    "            complex_word_count +=1\n",
    "        \n",
    "        if md.loc[word, 'Positive'] != 0:\n",
    "            \n",
    "            positive_score += 1\n",
    "        \n",
    "        if md.loc[word, 'Negative'] != 0:\n",
    "            \n",
    "            negative_score += 1\n",
    "    \n",
    "        if word in uncertainity_words:\n",
    "            \n",
    "            uncertainty_score += 1\n",
    "            \n",
    "        if word in constraining_words:\n",
    "            \n",
    "            constraining_score += 1\n",
    "            \n",
    "    polarity_score = (positive_score - negative_score)/((positive_score + negative_score) + 0.000001)\n",
    "    \n",
    "    subjectivity_score = (positive_score + negative_score)/(len(words) + 0.000001)\n",
    "\n",
    "    average_sentence_length = len(words)/number_of_sentences\n",
    "    \n",
    "    percentage_of_complex_words = complex_word_count / len(words)\n",
    "    \n",
    "    fog_index = (0.4)*(average_sentence_length + percentage_of_complex_words)\n",
    "    \n",
    "    average_word_length = word_length / len(words)\n",
    "    \n",
    "    positive_word_proportion = positive_score / word_count\n",
    "    \n",
    "    negative_word_proportion = negative_score / word_count\n",
    "    \n",
    "    uncertainty_word_proportion = uncertainty_score / word_count\n",
    "    \n",
    "    constraining_word_proportion = constraining_score / words_count\n",
    "    \n",
    "    return (positive_score, \n",
    "            negative_score, \n",
    "            average_sentence_length, \n",
    "            percentage_of_complex_words, \n",
    "            fog_index, \n",
    "            complex_word_count,\n",
    "            word_count,\n",
    "            uncertainty_score,\n",
    "            constraining_score,\n",
    "            positive_word_proportion,\n",
    "            negative_word_proportion,\n",
    "            uncertainty_word_proportion,\n",
    "            constraining_word_proportion\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_variables():\n",
    "    \n",
    "    for i in mda.keys():\n",
    "        (mda_positive_score[i],\n",
    "        mda_negative_score[i],\n",
    "        mda_polarity_score[i],\n",
    "        mda_average_sentence_length[i],\n",
    "        mda_percentage_of_complex_words[i],\n",
    "        mda_fog_index[i],\n",
    "        mda_complex_word_count[i],\n",
    "        mda_word_count[i],\n",
    "        mda_uncertainty_score[i],\n",
    "        mda_constraining_score[i],\n",
    "        mda_positive_word_proportion[i],\n",
    "        mda_negative_word_proportion[i],\n",
    "        mda_uncertainty_word_proportion[i],\n",
    "        mda_constraining_word_proportion[i]) = scores(mda[i])\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = urllib.request.urlopen(url)\n",
    "\n",
    "raw = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_filename, headers = urllib.request.urlretrieve('https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-000413.txt')\n",
    "# html = open(local_filename).decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sections = [\"Management's Discussion and Analysis\", \"Quantitative and Qualitative Disclosures about Market Risk\", \"Risk Factors\"]\n",
    "\n",
    "# z = html.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f= open(\"temp.txt\",\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.write(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('temp.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# print(f[:-1])\n",
    "        \n",
    "# f = open('temp_file.txt', 'w')\n",
    "\n",
    "# f.write(z)\n",
    "\n",
    "# k = open('temp_file.txt', 'r')\n",
    "\n",
    "# k.readlines(1)\n",
    "\n",
    "# k.close()\n",
    "\n",
    "# f.close()\n",
    "    \n",
    "#     print(type(html))\n",
    "            \n",
    "#     with html as fileinput:\n",
    "        \n",
    "#         print(fileinput)\n",
    "        \n",
    "#         exit\n",
    "        \n",
    "#         for line in fileinput:\n",
    "            \n",
    "#             line = line.lower()\n",
    "        \n",
    "#         if '\\n' in  fileinput:\n",
    "            \n",
    "#             print('yes')\n",
    "\n",
    "# f.close()\n",
    "\n",
    "# f = open('temp_file.txt' 'r')\n",
    "\n",
    "# f.read()\n",
    "\n",
    "# f.close()\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-000413.txt\"\n",
    "response = urllib.request.urlopen(url)\n",
    "raw = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex = r\"(?<=ITEM\\s\\d\\.\\sMANAGEMENT\\'S\\sDISCUSSION\\sAND\\sANALYSIS\\sOF\\sFINANCIAL\\sCONDITION\\sAND\\sRESULTS\\sOF\\sOPERATION)(.*?)(?=ITEM\\s\\d\\.\\s)\"\n",
    "\n",
    "#test_str = (\"ITEM 7. MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATION \\nkdfoks[dlsaf;\\nsd\\nlgsld\\ns['a\\n'[df\\na'fd\\nsaD\\FSAD\\nfF\\n ITEM 8. dhfkjskaskdkskd\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re.findall(regex, test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(l)):\n",
    "    l[i] = l[i].replace('\\n', ' ')\n",
    "    l[i] = l[i].replace('\\\\', '')\n",
    "    l[i] = l[i].replace('\\t', ' ')\n",
    "    l[i] = l[i].replace(\"\\'\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_str = ' '.join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern = r'''(?<=ITEM\\s\\d\\.\\sMANAGEMENTS\\sDISCUSSION\\sAND\\sANALYSIS\\sOF\\sFINANCIAL\\sCONDITION\\sAND\\sRESULTS\\sOF\\sOPERATION)(.*?)(?=ITEM\\s\\d\\.\\s)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'''(ITEM\\s\\d\\.\\sMANAGEMENTS\\sDISCUSSION\\sAND\\sANALYSIS\\sOF\\sFINANCIAL\\sCONDITION\\sAND\\sRESULTS)(.*?)(ITEM\\s\\d\\.\\s)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_list = re.findall(pattern, new_str, re.M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text = new_list[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_text = re.sub(r'\\s+', r' ', final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text = re.sub(r\"(<.*>)\",r\" \",final_text).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text = re.sub(\"[^a-zA-Z]\",\" \",final_text).lower()\n",
    "\n",
    "final_text\n",
    "\n",
    "final_text = re.sub(r'\\s+', r' ', final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_stop_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "stop_word_file_path = '/home/pramila/Desktop/CLASSWORK/ML-ASGN-BLACKCOFFER/blackcoffer assignment/STOP-WORDS/'\n",
    "\n",
    "list_of_files = os.listdir(stop_word_file_path)\n",
    "\n",
    "#2\n",
    "for i in list_of_files:\n",
    "    \n",
    "    with open(os.path.join(stop_word_file_path, i), 'r') as words :\n",
    "        \n",
    "        content = words.read()\n",
    "        \n",
    "        with open('stop_words.txt', 'a+') as stop_words :\n",
    "            \n",
    "            stop_words.write(content + '\\n')\n",
    "            \n",
    "            \n",
    "#3\n",
    "with open('stop_words.txt', 'r') as stop_words :\n",
    "            \n",
    "            list_of_stop_words = stop_words.readlines()\n",
    "\n",
    "#list_of_stop_words\n",
    "\n",
    "#some modification req in stop_words_list\n",
    "\n",
    "for i in range(len(list_of_stop_words)) :\n",
    "    \n",
    "    list_of_stop_words[i] = list_of_stop_words[i].replace('\\n', \"\").lower()\n",
    "\n",
    "\n",
    "list_of_stop_words = list(set(list_of_stop_words))\n",
    "\n",
    "list_of_stop_words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if polarity_score < -0.5 :\n",
    "        \n",
    "        sentiment_score_categorization = 'Most Negative'\n",
    "        \n",
    "    elif (polarity_score < 0) and (polarity_score > -0.5):\n",
    "        \n",
    "        sentiment_score_categorization = 'Negative'\n",
    "        \n",
    "    elif polarity_score == 0:\n",
    "        \n",
    "        sentiment_score_categorization = 'Neutral'\n",
    "        \n",
    "    elif (polarity_score > 0) and (polarity_score < 0.5):\n",
    "        \n",
    "        sentiment_score_categorization = 'Positive'\n",
    "        \n",
    "    elif polarity_score > 0.5:\n",
    "        \n",
    "        sentiment_score_categorization = 'Very Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_stop_words(content):\n",
    "#     new_content = content\n",
    "#     print(len(content))\n",
    "    \n",
    "#     print(\"\\n\")\n",
    "#     for i in range(len(content)):\n",
    "#         print(i)\n",
    "        \n",
    "#         if content[i] in li1:\n",
    "        \n",
    "#             del new_content[i]\n",
    "            \n",
    "#         elif content[i] in li2:\n",
    "            \n",
    "#             del new_content[i]\n",
    "            \n",
    "#         elif content[i] in li3:\n",
    "            \n",
    "#             del new_content[i]\n",
    "            \n",
    "#         elif content[i] in li4:\n",
    "            \n",
    "#             del new_content[i]\n",
    "            \n",
    "#     return new_content\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_string = \" \".join(list_of_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text = re.split(r' ', final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text = set(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import master dictionary\n",
    "\n",
    "# master_dict_ini = pd.read_excel('LoughranMcDonald_MasterDictionary_2018.xlsx')\n",
    "\n",
    "# md = master_dict_ini.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       86132\n",
       "2009      352\n",
       "2012        1\n",
       "2011        1\n",
       "Name: Positive, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md['Positive'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84131\n",
       "2009     2315\n",
       "2014       26\n",
       "2011       13\n",
       "2012        1\n",
       "Name: Negative, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md['Negative'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12of12inf    81536\n",
       "10K_2010      1898\n",
       "2018          1265\n",
       "10K_2008       871\n",
       "10K_2014       462\n",
       "10K_2012       339\n",
       "10K_2016        90\n",
       "10K_2009        14\n",
       "10K_2011        11\n",
       "Name: Source, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md['Source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = 'A Turning machine is a device that manipulates symbols on a strip of tape according to a table of rules. Despite its simplicity, a Turing machine can be adapted to simulate the logic of any computer algorithm, and is particularly useful in explaining the functions of a CPU.... inside a computer. The \"Turing\" machine was described by Alan Turing in 1936, who called it an \"a(utomatic)-machine\". The Turing machine is not intended as a practical computing technology, but rather as a hypothetical device representing a computing machine. Turing machines help computer scientists understand the limits of mechaniacl computation.'\n",
    "\n",
    "number_of_sentences = sent_tokenize(sentences)\n",
    "\n",
    "print(len(number_of_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "md['Word'] = md['Word'].apply(lambda x: str(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abs\n",
      "ace\n",
      "act\n",
      "add\n",
      "ado\n",
      "ads\n",
      "adz\n",
      "aft\n",
      "age\n",
      "ago\n",
      "aha\n",
      "aid\n",
      "ail\n",
      "aim\n",
      "air\n",
      "alb\n",
      "ale\n",
      "all\n",
      "alp\n",
      "amp\n",
      "and\n",
      "ani\n",
      "ant\n",
      "any\n",
      "ape\n",
      "apt\n",
      "arc\n",
      "are\n",
      "ark\n",
      "arm\n",
      "art\n",
      "ash\n",
      "ask\n",
      "asp\n",
      "ass\n",
      "ate\n",
      "auk\n",
      "awe\n",
      "awl\n",
      "awn\n",
      "axe\n",
      "aye\n",
      "baa\n",
      "bad\n",
      "bag\n",
      "bah\n",
      "ban\n",
      "bar\n",
      "bat\n",
      "bay\n",
      "bed\n",
      "bee\n",
      "beg\n",
      "bet\n",
      "bey\n",
      "bib\n",
      "bid\n",
      "big\n",
      "bin\n",
      "bio\n",
      "bis\n",
      "bit\n",
      "biz\n",
      "boa\n",
      "bob\n",
      "bod\n",
      "bog\n",
      "boo\n",
      "bop\n",
      "bow\n",
      "box\n",
      "boy\n",
      "bra\n",
      "bro\n",
      "brr\n",
      "bub\n",
      "bud\n",
      "bug\n",
      "bum\n",
      "bun\n",
      "bur\n",
      "bus\n",
      "but\n",
      "buy\n",
      "bye\n",
      "cab\n",
      "cad\n",
      "cam\n",
      "can\n",
      "cap\n",
      "car\n",
      "cat\n",
      "caw\n",
      "cay\n",
      "chi\n",
      "cob\n",
      "cod\n",
      "cog\n",
      "con\n",
      "coo\n",
      "cop\n",
      "cos\n",
      "cot\n",
      "cow\n",
      "coy\n",
      "cry\n",
      "cub\n",
      "cud\n",
      "cue\n",
      "cum\n",
      "cup\n",
      "cur\n",
      "cut\n",
      "dab\n",
      "dad\n",
      "dam\n",
      "day\n",
      "deb\n",
      "def\n",
      "den\n",
      "dew\n",
      "did\n",
      "die\n",
      "dig\n",
      "dim\n",
      "din\n",
      "dip\n",
      "dis\n",
      "doc\n",
      "doe\n",
      "dog\n",
      "doh\n",
      "don\n",
      "dos\n",
      "dot\n",
      "dry\n",
      "dub\n",
      "dud\n",
      "due\n",
      "dug\n",
      "duh\n",
      "dun\n",
      "duo\n",
      "dye\n",
      "ear\n",
      "eat\n",
      "ebb\n",
      "eds\n",
      "eek\n",
      "eel\n",
      "egg\n",
      "ego\n",
      "eke\n",
      "electroencephalographies\n",
      "elf\n",
      "elk\n",
      "ell\n",
      "elm\n",
      "ems\n",
      "emu\n",
      "end\n",
      "ens\n",
      "eon\n",
      "era\n",
      "ere\n",
      "erg\n",
      "err\n",
      "eta\n",
      "eve\n",
      "ewe\n",
      "eye\n",
      "fad\n",
      "fag\n",
      "fan\n",
      "far\n",
      "fas\n",
      "fat\n",
      "fax\n",
      "fay\n",
      "fed\n",
      "fee\n",
      "fen\n",
      "few\n",
      "fey\n",
      "fez\n",
      "fib\n",
      "fie\n",
      "fig\n",
      "fin\n",
      "fir\n",
      "fit\n",
      "fix\n",
      "flu\n",
      "fly\n",
      "fob\n",
      "foe\n",
      "fog\n",
      "fop\n",
      "for\n",
      "fox\n",
      "fro\n",
      "fry\n",
      "fun\n",
      "fur\n",
      "gab\n",
      "gad\n",
      "gag\n",
      "gal\n",
      "gap\n",
      "gar\n",
      "gas\n",
      "gay\n",
      "gee\n",
      "gel\n",
      "gem\n",
      "get\n",
      "gig\n",
      "gin\n",
      "gnu\n",
      "gob\n",
      "god\n",
      "goo\n",
      "got\n",
      "gum\n",
      "gun\n",
      "gut\n",
      "guy\n",
      "gym\n",
      "gyp\n",
      "had\n",
      "hag\n",
      "hah\n",
      "ham\n",
      "hap\n",
      "has\n",
      "hat\n",
      "haw\n",
      "hay\n",
      "hem\n",
      "hen\n",
      "hep\n",
      "her\n",
      "hes\n",
      "hew\n",
      "hex\n",
      "hey\n",
      "hid\n",
      "hie\n",
      "him\n",
      "hip\n",
      "his\n",
      "hit\n",
      "hmm\n",
      "hob\n",
      "hoc\n",
      "hod\n",
      "hoe\n",
      "hog\n",
      "hon\n",
      "hop\n",
      "hos\n",
      "hot\n",
      "how\n",
      "hub\n",
      "hue\n",
      "hug\n",
      "huh\n",
      "hum\n",
      "hut\n",
      "ice\n",
      "icy\n",
      "ids\n",
      "ifs\n",
      "ilk\n",
      "ill\n",
      "imp\n",
      "ink\n",
      "inn\n",
      "ins\n",
      "ion\n",
      "ire\n",
      "irk\n",
      "ism\n",
      "its\n",
      "ivy\n",
      "jab\n",
      "jag\n",
      "jam\n",
      "jar\n",
      "jaw\n",
      "jay\n",
      "jet\n",
      "jib\n",
      "jig\n",
      "job\n",
      "jog\n",
      "jot\n",
      "joy\n",
      "jug\n",
      "jut\n",
      "keg\n",
      "ken\n",
      "key\n",
      "kid\n",
      "kin\n",
      "kip\n",
      "kit\n",
      "lab\n",
      "lac\n",
      "lad\n",
      "lag\n",
      "lam\n",
      "lap\n",
      "las\n",
      "law\n",
      "lax\n",
      "lay\n",
      "lea\n",
      "led\n",
      "lee\n",
      "leg\n",
      "lei\n",
      "let\n",
      "lib\n",
      "lid\n",
      "lie\n",
      "lip\n",
      "lit\n",
      "lob\n",
      "log\n",
      "lop\n",
      "lot\n",
      "low\n",
      "lox\n",
      "lug\n",
      "lye\n",
      "mac\n",
      "mad\n",
      "mag\n",
      "man\n",
      "map\n",
      "mar\n",
      "mas\n",
      "mat\n",
      "maw\n",
      "max\n",
      "may\n",
      "men\n",
      "met\n",
      "mew\n",
      "mid\n",
      "mil\n",
      "mis\n",
      "mix\n",
      "mob\n",
      "mod\n",
      "mom\n",
      "moo\n",
      "mop\n",
      "mot\n",
      "mow\n",
      "mud\n",
      "mug\n",
      "mum\n",
      "mus\n",
      "nab\n",
      "nae\n",
      "nag\n",
      "nap\n",
      "nay\n",
      "nee\n",
      "net\n",
      "new\n",
      "nib\n",
      "nil\n",
      "nip\n",
      "nit\n",
      "nix\n",
      "nod\n",
      "nor\n",
      "nos\n",
      "not\n",
      "now\n",
      "nth\n",
      "nub\n",
      "nan\n",
      "nun\n",
      "nus\n",
      "nut\n",
      "oaf\n",
      "oak\n",
      "oar\n",
      "oat\n",
      "obi\n",
      "odd\n",
      "ode\n",
      "off\n",
      "oft\n",
      "ohm\n",
      "oho\n",
      "ohs\n",
      "oil\n",
      "old\n",
      "ole\n",
      "oms\n",
      "one\n",
      "ooh\n",
      "ope\n",
      "ops\n",
      "opt\n",
      "orb\n",
      "ore\n",
      "our\n",
      "out\n",
      "ova\n",
      "owe\n",
      "owl\n",
      "own\n",
      "pad\n",
      "pal\n",
      "pan\n",
      "pap\n",
      "par\n",
      "pas\n",
      "pat\n",
      "paw\n",
      "pay\n",
      "pea\n",
      "pee\n",
      "peg\n",
      "pen\n",
      "pep\n",
      "per\n",
      "pet\n",
      "pew\n",
      "phi\n",
      "pic\n",
      "pie\n",
      "pig\n",
      "pin\n",
      "pip\n",
      "pis\n",
      "pit\n",
      "pix\n",
      "ply\n",
      "pod\n",
      "poi\n",
      "pol\n",
      "pop\n",
      "pot\n",
      "pox\n",
      "pro\n",
      "pry\n",
      "psi\n",
      "pub\n",
      "pug\n",
      "pun\n",
      "pup\n",
      "pus\n",
      "put\n",
      "pyx\n",
      "qua\n",
      "que\n",
      "quo\n",
      "rad\n",
      "rag\n",
      "rah\n",
      "ram\n",
      "ran\n",
      "rap\n",
      "rat\n",
      "raw\n",
      "ray\n",
      "rec\n",
      "red\n",
      "ref\n",
      "rem\n",
      "rep\n",
      "res\n",
      "rev\n",
      "rho\n",
      "rib\n",
      "rid\n",
      "rig\n",
      "rim\n",
      "rip\n",
      "rob\n",
      "rod\n",
      "roe\n",
      "rot\n",
      "row\n",
      "rub\n",
      "rue\n",
      "rug\n",
      "rum\n",
      "run\n",
      "rut\n",
      "rye\n",
      "sac\n",
      "sad\n",
      "sag\n",
      "sap\n",
      "sat\n",
      "saw\n",
      "sax\n",
      "say\n",
      "sea\n",
      "sec\n",
      "see\n",
      "set\n",
      "sew\n",
      "sex\n",
      "she\n",
      "shh\n",
      "shy\n",
      "sic\n",
      "sin\n",
      "sip\n",
      "sir\n",
      "sis\n",
      "sit\n",
      "six\n",
      "ska\n",
      "ski\n",
      "sky\n",
      "sly\n",
      "sob\n",
      "sod\n",
      "sol\n",
      "son\n",
      "sop\n",
      "sos\n",
      "sot\n",
      "sou\n",
      "sow\n",
      "sox\n",
      "soy\n",
      "spa\n",
      "spy\n",
      "ssh\n",
      "sty\n",
      "sub\n",
      "sue\n",
      "sum\n",
      "sun\n",
      "sup\n",
      "tab\n",
      "tad\n",
      "tag\n",
      "tam\n",
      "tan\n",
      "tap\n",
      "tar\n",
      "tat\n",
      "tau\n",
      "tax\n",
      "tea\n",
      "tee\n",
      "ten\n",
      "the\n",
      "tho\n",
      "thy\n",
      "tic\n",
      "tie\n",
      "tin\n",
      "tip\n",
      "tis\n",
      "tit\n",
      "toe\n",
      "tog\n",
      "tom\n",
      "ton\n",
      "too\n",
      "top\n",
      "tor\n",
      "tot\n",
      "tow\n",
      "toy\n",
      "try\n",
      "tub\n",
      "tug\n",
      "tun\n",
      "tut\n",
      "tux\n",
      "two\n",
      "ugh\n",
      "ump\n",
      "ups\n",
      "urn\n",
      "use\n",
      "van\n",
      "vat\n",
      "veg\n",
      "vet\n",
      "vex\n",
      "via\n",
      "vie\n",
      "vim\n",
      "vow\n",
      "wad\n",
      "wag\n",
      "wan\n",
      "war\n",
      "was\n",
      "wax\n",
      "way\n",
      "web\n",
      "wed\n",
      "wee\n",
      "wen\n",
      "wet\n",
      "who\n",
      "why\n",
      "wig\n",
      "win\n",
      "wit\n",
      "wiz\n",
      "woe\n",
      "wok\n",
      "won\n",
      "woo\n",
      "wow\n",
      "wry\n",
      "xis\n",
      "yak\n",
      "yam\n",
      "yap\n",
      "yaw\n",
      "yea\n",
      "yen\n",
      "yep\n",
      "yes\n",
      "yet\n",
      "yew\n",
      "yin\n",
      "yip\n",
      "yon\n",
      "you\n",
      "yow\n",
      "yuk\n",
      "yum\n",
      "yup\n",
      "zap\n",
      "zed\n",
      "zip\n",
      "zit\n",
      "zoo\n"
     ]
    }
   ],
   "source": [
    "total_word_length = 0\n",
    "total_words = md['Word'].shape[0]\n",
    "all_lengths = []\n",
    "for i in md['Word']:\n",
    "    total_word_length += len(i)\n",
    "    all_lengths.append(len(i))\n",
    "    if (len(i) == 3 or len(i) == 24) :\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(all_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.786150359595773"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_word_length/total_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
